{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5953ed-8e78-453b-a570-b7f8b8d98681",
   "metadata": {},
   "source": [
    "# MLOps stage 2 : Experimentation: Vertex AI Training for XGBoost with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe529b-02be-4141-beab-1be13040ea63",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6c452-5db7-4713-971a-5b232e063f00",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. Here we are covering stage 2 : Vertex AI training for XGBoost with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec46cb6-727c-4e32-8175-c1c62d637d05",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42394b1-0a9c-4aa6-accb-0fc40b3ee3c9",
   "metadata": {},
   "source": [
    "In this tutorial, you learn how to use Vertex AI Training for training a XGBoost custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "- Vertex AI Training\n",
    "- Vertex AI Model resource\n",
    "- The steps performed include:\n",
    "- Vizier hyperparameter tuning\n",
    "\n",
    "Training using a Python package.\n",
    "- Report accuracy when hyperparameter tuning.\n",
    "- Save the model artifacts to Cloud Storage using GCSFuse.\n",
    "- Create a Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74267e6-70e3-418a-b985-fbbf4cfd5ac6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2374b-17ca-4baf-9b2d-2fa3cd7b8ab7",
   "metadata": {},
   "source": [
    "The dataset used in this example is the Synthetic Financial Fraud dataset from Kaggle. PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f4c93-9392-4769-8d26-c3e405890c58",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5794cd2c-5b51-44a5-aa53-f1657c839a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d718262-e405-46c9-8861-628b4810cb0f",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff030b4-7d3a-4dc9-8d96-96e79ce97057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce22e57-dc90-42db-ad36-fd654a2feb09",
   "metadata": {},
   "source": [
    "## Set up Project Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a410e3b5-c0c6-4660-baf6-87a0a15bddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"bq-experiments-350102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15cce36-7ae8-481a-86ac-f8ed01b2c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0abc11-babf-4977-a3f9-23905b4929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb680865-2cb9-4346-b1e4-310ab7e21425",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bq-experiments-fraud\" \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94dc6e2c-eaf0-4502-92f2-fc27c2f3fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 493534783  2022-08-25T16:24:56Z  gs://bq-experiments-fraud/synthetic-fraud.csv#1661444696515532  metageneration=1\n",
      "                                 gs://bq-experiments-fraud/pipelines/\n",
      "TOTAL: 1 objects, 493534783 bytes (470.67 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a0018-48ba-4612-a151-e6f0c8932e45",
   "metadata": {},
   "source": [
    "## Initialize AIP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ef6cfa-50fb-492e-8f47-7398fa3b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49239eb-3d62-441b-b76d-3f2ec19c37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2b0ed-4d37-41b2-beca-99e36f5e648b",
   "metadata": {},
   "source": [
    "## Set Hardware Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62dc0e4f-20b1-4aaf-8d45-d183359607db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b931e-dace-420c-917f-3aff5d705d64",
   "metadata": {},
   "source": [
    "## Set Pre-built Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2099cbe6-a1ad-4bf9-8d15-580d081e379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "DEPLOY_VERSION = \"xgboost-cpu.1-1\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b7226-22c4-4048-bc07-5d22c7c6d805",
   "metadata": {},
   "source": [
    "## Set Machine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "686dc733-867f-43b0-ab9d-7b6d84c62e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc14b0-659c-4108-919b-8a1d56f61113",
   "metadata": {},
   "source": [
    "# Scikit-learn Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d541a4-f151-4aea-b509-60d7a9d8ce13",
   "metadata": {},
   "source": [
    "Once you have trained a scikit-learn model, you will want to save it at a Cloud Storage location, so it can subsequently be uploaded to a Vertex AI Model resource. You will do the following steps to save to a Cloud Storage location.. \n",
    "- Save the in-memory model to the local filesystem in pickle format (e.g., model.pkl).\n",
    "- Create a Cloud Storage storage client.\n",
    "- Upload the pickle file as a blob to the specified Cloud Storage location using the Cloud Storage storage client.\n",
    "\n",
    "You can do hyperparameter tuning with a XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79442e1-1d71-496a-b772-3b311d442642",
   "metadata": {},
   "source": [
    "## Training Package Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bcc21-9d9b-440c-83c3-760aaa8aa184",
   "metadata": {},
   "source": [
    "**Package layout**\n",
    "Before you start the training, you need look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "- __init__.py\n",
    "- task.py\n",
    "\n",
    "The files setup.cfg and setup.py are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file trainer/task.py is the Python script for executing the custom training job. Note, when we referred to it in the worker pool specification, we replace the directory slash with a dot (trainer.task) and dropped the file suffix (.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fcf3b-6941-4d63-a331-8cf0fd53f649",
   "metadata": {},
   "source": [
    "## Package Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e599772-ce6f-4916-9b86-4d404571b1a1",
   "metadata": {},
   "source": [
    "The following cells will assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4943409e-0389-4b47-a5f7-c942a7e04c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Financial Fraud Classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: bryanfreeman@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e50913-94ab-49e5-9eb3-83b306a75176",
   "metadata": {},
   "source": [
    "## Create the task script for the training package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808776f3-bdc5-4909-96ba-b28b33ab5c6e",
   "metadata": {},
   "source": [
    "Next, we need to create the task.py script for driving the training package. Some noteable steps include:\n",
    "\n",
    "Command-line arguments:\n",
    "- model-dir: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: AIP_MODEL_DIR,\n",
    "- dataset_data_url: The location of the training data to download.\n",
    "- dataset_labels_url: The location of the training labels to download.\n",
    "- boost-rounds: Tunable hyperparameter\n",
    "\n",
    "Data preprocessing (get_data()):\n",
    "- Download the dataset and split into training and test.\n",
    "\n",
    "Training (train_model()):\n",
    "- Trains the model\n",
    "\n",
    "Evaluation (evaluate_model()):\n",
    "- Evaluates the model.\n",
    "- If hyperparameter tuning, reports the metric for accuracy.\n",
    "\n",
    "Model artifact saving\n",
    "- Saves the model artifacts and evaluation metrics where the Cloud Storage location specified by model-dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7e4eb-3a0a-4792-9ed0-ff0e814cc72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
