{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5953ed-8e78-453b-a570-b7f8b8d98681",
   "metadata": {},
   "source": [
    "# MLOps stage 2 : Experimentation: Vertex AI Training for XGBoost with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe529b-02be-4141-beab-1be13040ea63",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6c452-5db7-4713-971a-5b232e063f00",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. Here we are covering stage 2 : Vertex AI training for XGBoost with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec46cb6-727c-4e32-8175-c1c62d637d05",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42394b1-0a9c-4aa6-accb-0fc40b3ee3c9",
   "metadata": {},
   "source": [
    "In this tutorial, you learn how to use Vertex AI Training for training a XGBoost custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "- Vertex AI Training\n",
    "- Vertex AI Model resource\n",
    "- The steps performed include:\n",
    "- Vizier hyperparameter tuning\n",
    "\n",
    "Training using a Python package.\n",
    "- Report accuracy when hyperparameter tuning.\n",
    "- Save the model artifacts to Cloud Storage using GCSFuse.\n",
    "- Create a Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74267e6-70e3-418a-b985-fbbf4cfd5ac6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2374b-17ca-4baf-9b2d-2fa3cd7b8ab7",
   "metadata": {},
   "source": [
    "The dataset used in this example is the Synthetic Financial Fraud dataset from Kaggle. PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f4c93-9392-4769-8d26-c3e405890c58",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5794cd2c-5b51-44a5-aa53-f1657c839a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d718262-e405-46c9-8861-628b4810cb0f",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff030b4-7d3a-4dc9-8d96-96e79ce97057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce22e57-dc90-42db-ad36-fd654a2feb09",
   "metadata": {},
   "source": [
    "## Set up Project Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a410e3b5-c0c6-4660-baf6-87a0a15bddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"bq-experiments-350102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15cce36-7ae8-481a-86ac-f8ed01b2c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0abc11-babf-4977-a3f9-23905b4929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb680865-2cb9-4346-b1e4-310ab7e21425",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bq-experiments-fraud\" \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc6e2c-eaf0-4502-92f2-fc27c2f3fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 493534783  2022-08-25T16:24:56Z  gs://bq-experiments-fraud/synthetic-fraud.csv#1661444696515532  metageneration=1\n",
      "                                 gs://bq-experiments-fraud/pipelines/\n",
      "TOTAL: 1 objects, 493534783 bytes (470.67 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a0018-48ba-4612-a151-e6f0c8932e45",
   "metadata": {},
   "source": [
    "## Initialize AIP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ef6cfa-50fb-492e-8f47-7398fa3b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49239eb-3d62-441b-b76d-3f2ec19c37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2b0ed-4d37-41b2-beca-99e36f5e648b",
   "metadata": {},
   "source": [
    "## Set Hardware Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62dc0e4f-20b1-4aaf-8d45-d183359607db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b931e-dace-420c-917f-3aff5d705d64",
   "metadata": {},
   "source": [
    "## Set Pre-built Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2099cbe6-a1ad-4bf9-8d15-580d081e379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "DEPLOY_VERSION = \"xgboost-cpu.1-1\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b7226-22c4-4048-bc07-5d22c7c6d805",
   "metadata": {},
   "source": [
    "## Set Machine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686dc733-867f-43b0-ab9d-7b6d84c62e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc14b0-659c-4108-919b-8a1d56f61113",
   "metadata": {},
   "source": [
    "# Scikit-learn Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d541a4-f151-4aea-b509-60d7a9d8ce13",
   "metadata": {},
   "source": [
    "Once you have trained a scikit-learn model, you will want to save it at a Cloud Storage location, so it can subsequently be uploaded to a Vertex AI Model resource. You will do the following steps to save to a Cloud Storage location.. \n",
    "- Save the in-memory model to the local filesystem in pickle format (e.g., model.pkl).\n",
    "- Create a Cloud Storage storage client.\n",
    "- Upload the pickle file as a blob to the specified Cloud Storage location using the Cloud Storage storage client.\n",
    "\n",
    "You can do hyperparameter tuning with a XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79442e1-1d71-496a-b772-3b311d442642",
   "metadata": {},
   "source": [
    "## Training Package Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bcc21-9d9b-440c-83c3-760aaa8aa184",
   "metadata": {},
   "source": [
    "**Package layout**\n",
    "Before you start the training, you need look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "- __init__.py\n",
    "- task.py\n",
    "\n",
    "The files setup.cfg and setup.py are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file trainer/task.py is the Python script for executing the custom training job. Note, when we referred to it in the worker pool specification, we replace the directory slash with a dot (trainer.task) and dropped the file suffix (.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e681adb-4ac9-4c9e-81f1-8542e77f879d",
   "metadata": {},
   "source": [
    "## Create UUDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10e35e82-7bea-44a2-b131-962e2c3f3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fcf3b-6941-4d63-a331-8cf0fd53f649",
   "metadata": {},
   "source": [
    "## Package Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e599772-ce6f-4916-9b86-4d404571b1a1",
   "metadata": {},
   "source": [
    "The following cells will assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4943409e-0389-4b47-a5f7-c942a7e04c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Financial Fraud Classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: bryanfreeman@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e50913-94ab-49e5-9eb3-83b306a75176",
   "metadata": {},
   "source": [
    "## Create the task script for the training package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808776f3-bdc5-4909-96ba-b28b33ab5c6e",
   "metadata": {},
   "source": [
    "Next, we need to create the task.py script for driving the training package. Some noteable steps include:\n",
    "\n",
    "Command-line arguments:\n",
    "- model-dir: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: AIP_MODEL_DIR,\n",
    "- dataset_data_url: The location of the training data to download.\n",
    "- dataset_labels_url: The location of the training labels to download.\n",
    "- boost-rounds: Tunable hyperparameter\n",
    "\n",
    "Data preprocessing (get_data()):\n",
    "- Download the dataset and split into training and test.\n",
    "\n",
    "Training (train_model()):\n",
    "- Trains the model\n",
    "\n",
    "Evaluation (evaluate_model()):\n",
    "- Evaluates the model.\n",
    "- If hyperparameter tuning, reports the metric for accuracy.\n",
    "\n",
    "Model artifact saving\n",
    "- Saves the model artifacts and evaluation metrics where the Cloud Storage location specified by model-dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b6f7e4eb-3a0a-4792-9ed0-ff0e814cc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import hypertune\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# SET UP TRAINING SCRIPT ARGUMENTS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument(\"--project-id\", dest=\"project_id\",\n",
    "                    type=str, help=\"Project id for bigquery client.\")\n",
    "parser.add_argument(\"--bq-table\", dest=\"bq_table\",\n",
    "                    type=str, help=\"Download url for the training data.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Function to retrieve data from BigQuery\n",
    "def get_data():\n",
    "    logging.info(\"Downloading training data from BigQuery: {}, {}\".format(args.project_id, args.bq_table))\n",
    "    logging.info(\"Creating BigQuery client\")\n",
    "    bqclient = bigquery.Client(project=args.project_id)\n",
    "    \n",
    "    logging.info(\"Loading table data\")\n",
    "    table = bigquery.TableReference.from_string(args.bq_table)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    dataframe = rows.to_dataframe()\n",
    "    \n",
    "    logging.info(\"Preparing data for training\")\n",
    "    dataframe[\"isFraud\"] = dataframe[\"isFraud\"].astype(int)\n",
    "    dataframe.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1,inplace=True)\n",
    "    X = pd.concat([dataframe.drop('type', axis=1), pd.get_dummies(dataframe['type'])], axis=1)\n",
    "    y = X[['isFraud']]\n",
    "    X = X.drop(['isFraud'],axis=1)\n",
    "    \n",
    "    logging.info(\"Splitting data for training\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42, shuffle=True)\n",
    "    \n",
    "    logging.info(\"Finishing get_data\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(X_train, y_train):\n",
    "    logging.info(\"Start training ...\")\n",
    "    model = xgb.XGBClassifier(scale_pos_weight=734)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    logging.info(\"Training completed\")\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    logging.info(\"Preparing test data ...\")\n",
    "    data_test = xgb.DMatrix(X_test)\n",
    "    \n",
    "    logging.info(\"Getting test predictions ...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    logging.info(\"Evaluating predictions ...\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    logging.info(f\"Evaluation completed with model specificity: {specificity}\")\n",
    "\n",
    "    logging.info(\"Report metric for hyperparameter tuning ...\")\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='specificity',\n",
    "        metric_value=specificity\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Finishing ...\")\n",
    "    return specificity\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()\n",
    "model = train_model(X_train, y_train)\n",
    "specificity = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# GCSFuse conversion\n",
    "gs_prefix = 'gs://'\n",
    "gcsfuse_prefix = '/gcs/'\n",
    "if args.model_dir.startswith(gs_prefix):\n",
    "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    dirpath = os.path.split(args.model_dir)[0]\n",
    "    if not os.path.isdir(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# Export the classifier to a file\n",
    "gcs_model_path = os.path.join(args.model_dir, 'model.bst')\n",
    "logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
    "model.save_model(gcs_model_path)\n",
    "\n",
    "logging.info(\"Saving metrics to {}/metrics.json\". format(args.model_dir))\n",
    "gcs_metrics_path = os.path.join(args.model_dir, 'metrics.json')\n",
    "with open(gcs_metrics_path, \"w\") as f:\n",
    "    f.write(f\"{'specificity: {specificity}'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9015c-103c-4049-851c-22a36498dc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ccfe45f-c0b1-4349-9e97-2ae7b0b222de",
   "metadata": {},
   "source": [
    "## Store Training Script to Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9858995b-ed3b-4f48-941b-470d940842b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/setup.py\n",
      "custom/setup.cfg\n",
      "custom/README.md\n",
      "custom/PKG-INFO\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_fraud.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba99b49-96b2-492c-80ae-9bf8d93f6ed5",
   "metadata": {},
   "source": [
    "## Create Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a68bdbba-cf95-43d6-a658-97e26317157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"fraud_\" + UUID\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_fraud.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813acfd-4966-4748-80b5-c0d194f95601",
   "metadata": {},
   "source": [
    "## Prepare Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e8cfa5c7-36e0-44cd-af92-538a461aa55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, UUID)\n",
    "BQ_TABLE = \"bq-experiments-350102.synthetic_financial_fraud.fraud_data\"\n",
    "ROUNDS = 20\n",
    "\n",
    "DIRECT = False\n",
    "if DIRECT:\n",
    "    CMDARGS = [\n",
    "        \"--project-id=\" + PROJECT_ID,\n",
    "        \"--bq-table=\" + BQ_TABLE,\n",
    "        \"--model_dir=\" + MODEL_DIR,\n",
    "    ]\n",
    "else:\n",
    "    CMDARGS = [\n",
    "        \"--project-id=\" + PROJECT_ID,\n",
    "        \"--bq-table=\" + BQ_TABLE,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7339ff5-99b3-48b5-8640-b7531fa5e0e8",
   "metadata": {},
   "source": [
    "## Run Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d9f49866-0e3a-4cf9-8523-834f622c5e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://bq-experiments-fraud/q0pjoruv \n"
     ]
    }
   ],
   "source": [
    "if TRAIN_GPU:\n",
    "    model = job.run(\n",
    "        model_display_name=\"fraud_\" + UUID,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "else:\n",
    "    model = job.run(\n",
    "        model_display_name=\"fraud_\" + UUID,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "model_path_to_deploy = MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98195658-f455-46b5-9621-a25824f02d20",
   "metadata": {},
   "source": [
    "## List Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "190f0b54-d138-47aa-8974-0b8a76d26f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/324854833595023360?project=402374189238\n",
      "[<google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f2dc42b9c10> \n",
      "resource name: projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360, <google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f2dc42245d0> \n",
      "resource name: projects/402374189238/locations/us-central1/trainingPipelines/9030875863255613440, <google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f2dc420c410> \n",
      "resource name: projects/402374189238/locations/us-central1/trainingPipelines/1616824976696934400, <google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f2dc4211210> \n",
      "resource name: projects/402374189238/locations/us-central1/trainingPipelines/4118574569701244928, <google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f2dc425b150> \n",
      "resource name: projects/402374189238/locations/us-central1/trainingPipelines/4144980440953913344]\n"
     ]
    }
   ],
   "source": [
    "_job = job.list(filter=f\"display_name={DISPLAY_NAME}\")\n",
    "print(_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d112668-ac2f-4379-a9c4-4fcd8b6c8916",
   "metadata": {},
   "source": [
    "## Wait for Custom Training Job to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0215a210-f70a-4027-a1cf-4acf4853d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6015944611149643776?project=402374189238\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob run completed. Resource name: projects/402374189238/locations/us-central1/trainingPipelines/324854833595023360\n",
      "Model available at projects/402374189238/locations/us-central1/models/1042156703224692736\n"
     ]
    }
   ],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc333490-e858-406a-bea6-32084b02f671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
