{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5953ed-8e78-453b-a570-b7f8b8d98681",
   "metadata": {},
   "source": [
    "# MLOps stage 2 : Experimentation: Vertex AI Training for XGBoost with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe529b-02be-4141-beab-1be13040ea63",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6c452-5db7-4713-971a-5b232e063f00",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. Here we are covering stage 2 : Vertex AI training for XGBoost with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec46cb6-727c-4e32-8175-c1c62d637d05",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42394b1-0a9c-4aa6-accb-0fc40b3ee3c9",
   "metadata": {},
   "source": [
    "In this tutorial, you learn how to use Vertex AI Training for training a XGBoost custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "- Vertex AI Training\n",
    "- Vertex AI Model resource\n",
    "- The steps performed include:\n",
    "- Vizier hyperparameter tuning\n",
    "\n",
    "Training using a Python package.\n",
    "- Report accuracy when hyperparameter tuning.\n",
    "- Save the model artifacts to Cloud Storage using GCSFuse.\n",
    "- Create a Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74267e6-70e3-418a-b985-fbbf4cfd5ac6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2374b-17ca-4baf-9b2d-2fa3cd7b8ab7",
   "metadata": {},
   "source": [
    "The dataset used in this example is the Synthetic Financial Fraud dataset from Kaggle. PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f4c93-9392-4769-8d26-c3e405890c58",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5794cd2c-5b51-44a5-aa53-f1657c839a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d718262-e405-46c9-8861-628b4810cb0f",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff030b4-7d3a-4dc9-8d96-96e79ce97057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce22e57-dc90-42db-ad36-fd654a2feb09",
   "metadata": {},
   "source": [
    "## Set up Project Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a410e3b5-c0c6-4660-baf6-87a0a15bddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"bq-experiments-350102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15cce36-7ae8-481a-86ac-f8ed01b2c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0abc11-babf-4977-a3f9-23905b4929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb680865-2cb9-4346-b1e4-310ab7e21425",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bq-experiments-fraud\" \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc6e2c-eaf0-4502-92f2-fc27c2f3fbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 493534783  2022-08-25T16:24:56Z  gs://bq-experiments-fraud/synthetic-fraud.csv#1661444696515532  metageneration=1\n",
      "      2139  2022-11-10T21:56:48Z  gs://bq-experiments-fraud/trainer_fraud.tar.gz#1668117408593300  metageneration=1\n",
      "                                 gs://bq-experiments-fraud/pipelines/\n",
      "                                 gs://bq-experiments-fraud/q0pjoruv/\n",
      "                                 gs://bq-experiments-fraud/vy5rkufq/\n",
      "TOTAL: 2 objects, 493536922 bytes (470.67 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a0018-48ba-4612-a151-e6f0c8932e45",
   "metadata": {},
   "source": [
    "## Initialize AIP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ef6cfa-50fb-492e-8f47-7398fa3b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49239eb-3d62-441b-b76d-3f2ec19c37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2b0ed-4d37-41b2-beca-99e36f5e648b",
   "metadata": {},
   "source": [
    "## Set Hardware Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62dc0e4f-20b1-4aaf-8d45-d183359607db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b931e-dace-420c-917f-3aff5d705d64",
   "metadata": {},
   "source": [
    "## Set Pre-built Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2099cbe6-a1ad-4bf9-8d15-580d081e379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "DEPLOY_VERSION = \"xgboost-cpu.1-1\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b7226-22c4-4048-bc07-5d22c7c6d805",
   "metadata": {},
   "source": [
    "## Set Machine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686dc733-867f-43b0-ab9d-7b6d84c62e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc14b0-659c-4108-919b-8a1d56f61113",
   "metadata": {},
   "source": [
    "# XGBoost Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d541a4-f151-4aea-b509-60d7a9d8ce13",
   "metadata": {},
   "source": [
    "Next, set up the package for XGBoost training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79442e1-1d71-496a-b772-3b311d442642",
   "metadata": {},
   "source": [
    "## Training Package Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bcc21-9d9b-440c-83c3-760aaa8aa184",
   "metadata": {},
   "source": [
    "**Package layout**\n",
    "Before you start the training, you need look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "    - __init__.py\n",
    "    - task.py\n",
    "\n",
    "The files setup.cfg and setup.py are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file trainer/task.py is the Python script for executing the custom training job. Note, when we referred to it in the worker pool specification, we replace the directory slash with a dot (trainer.task) and dropped the file suffix (.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e681adb-4ac9-4c9e-81f1-8542e77f879d",
   "metadata": {},
   "source": [
    "## Create UUDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e35e82-7bea-44a2-b131-962e2c3f3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fcf3b-6941-4d63-a331-8cf0fd53f649",
   "metadata": {},
   "source": [
    "## Package Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e599772-ce6f-4916-9b86-4d404571b1a1",
   "metadata": {},
   "source": [
    "The following cells will assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4943409e-0389-4b47-a5f7-c942a7e04c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Financial Fraud Classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: bryanfreeman@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e50913-94ab-49e5-9eb3-83b306a75176",
   "metadata": {},
   "source": [
    "## Create the task script for the training package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808776f3-bdc5-4909-96ba-b28b33ab5c6e",
   "metadata": {},
   "source": [
    "Next, we need to create the task.py script for driving the training package. Some noteable steps include:\n",
    "\n",
    "Command-line arguments:\n",
    "- model-dir: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: AIP_MODEL_DIR,\n",
    "- dataset_data_url: The location of the training data to download.\n",
    "- dataset_labels_url: The location of the training labels to download.\n",
    "- boost-rounds: Tunable hyperparameter\n",
    "\n",
    "Data preprocessing (get_data()):\n",
    "- Download the dataset and split into training and test.\n",
    "\n",
    "Training (train_model()):\n",
    "- Trains the model\n",
    "\n",
    "Evaluation (evaluate_model()):\n",
    "- Evaluates the model.\n",
    "- If hyperparameter tuning, reports the metric for accuracy.\n",
    "\n",
    "Model artifact saving\n",
    "- Saves the model artifacts and evaluation metrics where the Cloud Storage location specified by model-dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f7e4eb-3a0a-4792-9ed0-ff0e814cc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import hypertune\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# SET UP TRAINING SCRIPT ARGUMENTS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument(\"--project-id\", dest=\"project_id\",\n",
    "                    type=str, help=\"Project id for bigquery client.\")\n",
    "parser.add_argument(\"--bq-table\", dest=\"bq_table\",\n",
    "                    type=str, help=\"Download url for the training data.\")\n",
    "parser.add_argument('--max-depth', dest=\"max_depth\",\n",
    "                    type=int, help='Max depth of XGB tree', default=3)\n",
    "parser.add_argument('--learning-rate', dest=\"learning_rate\",\n",
    "                    type=float, help='Learning rate of XGB model', default=0.1)\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Function to retrieve data from BigQuery\n",
    "def get_data():\n",
    "    logging.info(\"Downloading training data from BigQuery: {}, {}\".format(args.project_id, args.bq_table))\n",
    "    logging.info(\"Creating BigQuery client\")\n",
    "    bqclient = bigquery.Client(project=args.project_id)\n",
    "    \n",
    "    logging.info(\"Loading table data\")\n",
    "    table = bigquery.TableReference.from_string(args.bq_table)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    dataframe = rows.to_dataframe()\n",
    "    \n",
    "    logging.info(\"Preparing data for training\")\n",
    "    dataframe[\"isFraud\"] = dataframe[\"isFraud\"].astype(int)\n",
    "    dataframe.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1,inplace=True)\n",
    "    X = pd.concat([dataframe.drop('type', axis=1), pd.get_dummies(dataframe['type'])], axis=1)\n",
    "    y = X[['isFraud']]\n",
    "    X = X.drop(['isFraud'],axis=1)\n",
    "    \n",
    "    logging.info(\"Splitting data for training\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42, shuffle=True)\n",
    "    \n",
    "    logging.info(\"Finishing get_data\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(X_train, y_train):\n",
    "    logging.info(\"Start training ...\")\n",
    "    model = xgb.XGBClassifier(\n",
    "            scale_pos_weight=734,\n",
    "            max_depth=args.max_depth,\n",
    "            learning_rate=args.learning_rate\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    logging.info(\"Training completed\")\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    logging.info(\"Preparing test data ...\")\n",
    "    data_test = xgb.DMatrix(X_test)\n",
    "    \n",
    "    logging.info(\"Getting test predictions ...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    logging.info(\"Evaluating predictions ...\")\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    logging.info(f\"Evaluation completed with weighted f1 score: {f1}\")\n",
    "\n",
    "    logging.info(\"Report metric for hyperparameter tuning ...\")\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='f1_score',\n",
    "        metric_value=f1\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Finishing ...\")\n",
    "    return f1\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data()\n",
    "model = train_model(X_train, y_train)\n",
    "f1 = evaluate_model(model, X_test, y_test)\n",
    "metric_dict = {'f1_score': f1}\n",
    "\n",
    "# GCSFuse conversion\n",
    "gs_prefix = 'gs://'\n",
    "gcsfuse_prefix = '/gcs/'\n",
    "if args.model_dir.startswith(gs_prefix):\n",
    "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    dirpath = os.path.split(args.model_dir)[0]\n",
    "    if not os.path.isdir(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# Export the classifier to a file\n",
    "gcs_model_path = os.path.join(args.model_dir, 'model.bst')\n",
    "logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
    "model.save_model(gcs_model_path)\n",
    "\n",
    "logging.info(\"Saving metrics to {}/metrics.json\". format(args.model_dir))\n",
    "gcs_metrics_path = os.path.join(args.model_dir, 'metrics.json')\n",
    "with open(gcs_metrics_path, \"w\") as f:\n",
    "    f.write(json.dumps(metric_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfe45f-c0b1-4349-9e97-2ae7b0b222de",
   "metadata": {},
   "source": [
    "## Store Training Script to Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9858995b-ed3b-4f48-941b-470d940842b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/setup.py\n",
      "custom/setup.cfg\n",
      "custom/README.md\n",
      "custom/PKG-INFO\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.1 KiB/  2.1 KiB]                                                \n",
      "Operation completed over 1 objects/2.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_fraud.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050e559-1532-4156-946c-8fe1a375783d",
   "metadata": {},
   "source": [
    "## Create Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0af2059-9363-49f3-8bb7-ac28b1ea3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"fraud_\" + UUID\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_fraud.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeccbf-6021-457c-a62a-b2fea8449729",
   "metadata": {},
   "source": [
    "## Prepare Training Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8423bfd-7322-42f3-84a9-488475840264",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, UUID)\n",
    "BQ_TABLE = \"bq-experiments-350102.synthetic_financial_fraud.fraud_data\"\n",
    "ROUNDS = 20\n",
    "\n",
    "DIRECT = False\n",
    "if DIRECT:\n",
    "    CMDARGS = [\n",
    "        \"--project-id=\" + PROJECT_ID,\n",
    "        \"--bq-table=\" + BQ_TABLE,\n",
    "        \"--model_dir=\" + MODEL_DIR,\n",
    "    ]\n",
    "else:\n",
    "    CMDARGS = [\n",
    "        \"--project-id=\" + PROJECT_ID,\n",
    "        \"--bq-table=\" + BQ_TABLE,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825ebb1-58fd-4bf4-af2f-cdd41b2ea57e",
   "metadata": {},
   "source": [
    "## Run Custom Traiing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f16d302-a5ea-4516-a6ef-0490c3b98599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://bq-experiments-fraud/mqmcvfd2 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/671315345553752064?project=402374189238\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2407030789440012288?project=402374189238\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_GPU:\n",
    "    model = job.run(\n",
    "        model_display_name=\"fraud_\" + UUID,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "else:\n",
    "    model = job.run(\n",
    "        model_display_name=\"fraud_\" + UUID,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "model_path_to_deploy = MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aaa9ea-4574-4de6-bfa4-1d67725046f5",
   "metadata": {},
   "source": [
    "## Wait for Training Job to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36918d90-04e5-4920-8894-93edd0ff75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob run completed. Resource name: projects/402374189238/locations/us-central1/trainingPipelines/671315345553752064\n",
      "Model available at projects/402374189238/locations/us-central1/models/3123945630976704512\n"
     ]
    }
   ],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264621cf-5917-4723-a491-688e5ac5f909",
   "metadata": {},
   "source": [
    "## Prepare Machine Specification for Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967eb1e7-ac9a-4b71-a3be-f52906a92924",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_GPU:\n",
    "    machine_spec = {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "    }\n",
    "else:\n",
    "    machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f5f5f-890a-4ea0-ae87-ed5d32ab5218",
   "metadata": {},
   "source": [
    "## Prepare Disk Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28e4d736-5b24-423e-8526-29a4a5c89918",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
    "DISK_SIZE = 200  # GB\n",
    "\n",
    "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813acfd-4966-4748-80b5-c0d194f95601",
   "metadata": {},
   "source": [
    "## Prepare Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8cfa5c7-36e0-44cd-af92-538a461aa55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, UUID)\n",
    "BQ_TABLE = \"bq-experiments-350102.synthetic_financial_fraud.fraud_data\"\n",
    "MAX_DEPTH = '3'\n",
    "LEARNING_RATE = '0.1'\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--project-id=\" + PROJECT_ID,\n",
    "    \"--bq-table=\" + BQ_TABLE,\n",
    "    \"--max-depth=\" + MAX_DEPTH,\n",
    "    \"--learning-rate=\" + LEARNING_RATE,\n",
    "    \"--model-dir=\" + MODEL_DIR,\n",
    "]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_URI + \"/trainer_fraud.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba99b49-96b2-492c-80ae-9bf8d93f6ed5",
   "metadata": {},
   "source": [
    "## Create Custom Job\n",
    "\n",
    "Use the class **CustomJob** to create a custom job, such as for hyperparameter tuning, with the following parameters:\n",
    "- **display_name**: A human readable name for the custom job.\n",
    "- **worker_pool_specs**: The specification for the corresponding VM instances.\n",
    "- **base_output_dir:** The Cloud Storage location for storing the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68bdbba-cf95-43d6-a658-97e26317157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"fraud_\" + UUID\n",
    "\n",
    "job = aip.CustomJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    worker_pool_specs=worker_pool_spec,\n",
    "    base_output_dir=MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270d106-324c-400b-acaa-39fadb266867",
   "metadata": {},
   "source": [
    "## Create a hyperparameter tuning job\n",
    "\n",
    "Use the class **HyperparameterTuningJob** to create a hyperparameter tuning job, with the following parameters:\n",
    "- **display_name:** A human readable name for the custom job.\n",
    "- **custom_job:** The worker pool spec from this custom job applies to the CustomJobs created in all the trials.\n",
    "- **metrics_spec:** The metrics to optimize. The dictionary key is the metric_id, which is reported by your training job, and the dictionary value is the optimization goal of the metric('minimize' or 'maximize').\n",
    "- **parameter_spec:** The parameters to optimize. The dictionary key is the metric_id, which is passed into your training job as a command line key word argument, and the dictionary value is the parameter specification of the metric.\n",
    "- **search_algorithm:** The search algorithm to use: grid, random and None. If None is specified, the Vizier service (Bayesian) is used.\n",
    "- **max_trial_count:** The maximum number of trials to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f75c177-a022-4fc6-b716-1d3f0b42e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "hpt_job = aip.HyperparameterTuningJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    custom_job=job,\n",
    "    metric_spec={\n",
    "        \"f1_score\": \"maximize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"max-depth\": hpt.IntegerParameterSpec(min=3, max=8, scale=\"linear\"),\n",
    "        \"learning-rate\": hpt.DoubleParameterSpec(min=0.01, max=0.1, scale='log'),\n",
    "    },\n",
    "    search_algorithm=None,\n",
    "    max_trial_count=6,\n",
    "    parallel_trial_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7339ff5-99b3-48b5-8640-b7531fa5e0e8",
   "metadata": {},
   "source": [
    "## Run Custom Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f49866-0e3a-4cf9-8523-834f622c5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98195658-f455-46b5-9621-a25824f02d20",
   "metadata": {},
   "source": [
    "## Display Job Trial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "190f0b54-d138-47aa-8974-0b8a76d26f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[id: \"1\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.0316227766016838\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 6.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9924660433969206\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668180989\n",
      "  nanos: 802552832\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668182156\n",
      "}\n",
      ", id: \"2\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.0524800915660787\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 4.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9905519850390407\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668182403\n",
      "  nanos: 112375048\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668183405\n",
      "}\n",
      ", id: \"3\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.01819702701033967\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 7.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9916669055272429\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668183645\n",
      "  nanos: 871540079\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668185018\n",
      "}\n",
      ", id: \"4\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.010366876413336713\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 6.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9888190251733115\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668185265\n",
      "  nanos: 291363675\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668186433\n",
      "}\n",
      ", id: \"5\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.02907839124678422\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 7.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9936826291775886\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668186659\n",
      "  nanos: 700841027\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668187915\n",
      "}\n",
      ", id: \"6\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"learning-rate\"\n",
      "  value {\n",
      "    number_value: 0.03289820323933852\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max-depth\"\n",
      "  value {\n",
      "    number_value: 7.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"f1_score\"\n",
      "    value: 0.9940546509962106\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1668188148\n",
      "  nanos: 636351467\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1668189468\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(hpt_job.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d112668-ac2f-4379-a9c4-4fcd8b6c8916",
   "metadata": {},
   "source": [
    "## Best Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0215a210-f70a-4027-a1cf-4acf4853d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6', 0.03289820323933852, 7.0, 0.9940546509962106)\n"
     ]
    }
   ],
   "source": [
    "best = (None, None, None, 0.0)\n",
    "for trial in hpt_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "        try:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                float(trial.parameters[1].value),\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "        except:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                None,\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bc6b2-f2f1-453d-ba16-8d98b793220a",
   "metadata": {},
   "source": [
    "## Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f014635-df02-4e57-8fde-4cb0b8d3f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_DIR = MODEL_DIR + \"/\" + best[0] + \"/model\"\n",
    "\n",
    "! gsutil ls {BEST_MODEL_DIR}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
