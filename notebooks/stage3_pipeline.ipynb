{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81ab15f-f995-4ea8-a96f-0d2f16613ac2",
   "metadata": {},
   "source": [
    "# MLOps Stage 3: Automation: Creating a Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd633a-0918-48df-abd1-78e36c3a7cc8",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d8232-1195-4a3a-908a-6a1172ac6455",
   "metadata": {},
   "source": [
    "In this notebook, we create a Vertex AI Pipeline for training and deploying a XGBoost model, and using Vertex AI Experiments to log training parameters and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f68c2c-526d-4a86-9027-8ee6073c8479",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cfe6d-c7d0-430f-92da-37fe1dc0ca11",
   "metadata": {},
   "source": [
    "Here, we use prebuilt components in Vertex AI Pipelines for training and deploying a XGBoost custom model, and using Vertex AI Experiments to log the corresponding training parameters and metrics, from within the training package.\n",
    "\n",
    "This notebook uses the following Google Cloud ML services:\n",
    "- Google Cloud Pipeline Components\n",
    "- Vertex AI Training\n",
    "- Vertex AI Pipelines\n",
    "- Vertex AI Experiments\n",
    "\n",
    "The steps performed include:\n",
    "- Construct a XGBoost training package.\n",
    "- Add tracking the experiment\n",
    "    - Construct a pipeline to train and deploy a XGBoost model.\n",
    "- Execute the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894cf5d-4f27-4836-b97d-4ddf435b8776",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec434f-8933-40a8-9d39-e13e60cb1183",
   "metadata": {},
   "source": [
    "The dataset used in this example is the Synthetic Financial Fraud dataset from Kaggle. PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c26dd5-5c11-4f42-90c8-18b55961c8f4",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bee4c-bbe4-4d9a-979b-63c182644193",
   "metadata": {},
   "source": [
    "Install the following packages for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018a1a14-6c84-4948-ad70-0f1992e3ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install {USER_FLAG} --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                             google-cloud-pipeline-components \\\n",
    "                                             kfp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f76ee-8169-41a7-bee5-78a6e4904cc9",
   "metadata": {},
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48567db6-06b8-45db-8082-1cab2c58963d",
   "metadata": {},
   "source": [
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633b44c2-2043-4296-91a2-38f140b9a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a821da-f7c5-4411-939e-93a35c88147d",
   "metadata": {},
   "source": [
    "## Set up Project Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18fe9e8-561a-4562-8482-d62b133d74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"bq-experiments-350102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d40dd51-4c2b-4c5c-90c9-0be1105b51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ebc409-6672-4395-a8b0-8b8c5c352efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c12512-3616-40f7-b13c-64f154799efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bq-experiments-fraud\" \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08314e2-6bf4-4dee-89e9-2b8da964df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 493534783  2022-08-25T16:24:56Z  gs://bq-experiments-fraud/synthetic-fraud.csv#1661444696515532  metageneration=1\n",
      "      2133  2022-11-11T15:17:22Z  gs://bq-experiments-fraud/trainer_fraud.tar.gz#1668179842539274  metageneration=1\n",
      "                                 gs://bq-experiments-fraud/mqmcvfd2/\n",
      "                                 gs://bq-experiments-fraud/pipelines/\n",
      "                                 gs://bq-experiments-fraud/q0pjoruv/\n",
      "                                 gs://bq-experiments-fraud/vy5rkufq/\n",
      "TOTAL: 2 objects, 493536916 bytes (470.67 MiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73090c-b4b9-4af5-8574-0003a5caab07",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91da6d95-70a2-40a6-8021-bc47576597b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 16:35:56.180224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 16:36:01.374886: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 16:36:09.320917: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-11-15 16:36:09.321334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-11-15 16:36:09.321356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "import tensorflow as tf\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b668fb-5db1-4caa-a8ca-cff47d050481",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8254131-70ac-4bb9-a26b-e436b1f4ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67cacd-6282-4c45-9829-2813211e306c",
   "metadata": {},
   "source": [
    "## Set Pre-built Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c335f3-9a92-42ae-9902-16dd93cd115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\n",
      "us-docker.pkg.dev/vertex-ai/serving/xgboost-cpu.1-1:latest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "DEPLOY_VERSION = \"xgboost-cpu.1-1\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/serving/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "\n",
    "print(TRAIN_IMAGE)\n",
    "print(DEPLOY_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0451f57-48f9-4d42-a60b-e3604f73d58b",
   "metadata": {},
   "source": [
    "## Set Machine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc0ef92-bb93-4915-ac2e-7aabd0e97565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffb2f6-0321-42bb-a9c5-4eefdf57bd3d",
   "metadata": {},
   "source": [
    "## XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d779131-e3be-4d88-9b4b-e8beabd7b5ab",
   "metadata": {},
   "source": [
    "### Package Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b13553d8-7c8b-4328-a9a6-befd8d897d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Financial Fraud Classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: bryanfreeman@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1e501-ea19-4bef-b1a1-c638bba0d40f",
   "metadata": {},
   "source": [
    "### Create Task Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1017f192-9cec-4f89-83e6-5f7d1941b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import subprocess\n",
    "subprocess.check_call(['pip3', 'install', \"--upgrade\",\n",
    "                       \"google-cloud-aiplatform\", \n",
    "                       \"google-cloud-bigquery\"], \n",
    "                      stderr=sys.stdout\n",
    "                     )\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "# SET UP TRAINING SCRIPT ARGUMENTS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument(\"--project-id\", dest=\"project_id\",\n",
    "                    type=str, help=\"Project id for bigquery client.\")\n",
    "parser.add_argument(\"--bq-table\", dest=\"bq_table\",\n",
    "                    type=str, help=\"Table location the training data.\")\n",
    "\n",
    "# Args for experiment\n",
    "parser.add_argument('--experiment', dest='experiment',\n",
    "                    required=True, type=str,\n",
    "                    help='Name of experiment')\n",
    "parser.add_argument('--run', dest='run',\n",
    "                    required=True, type=str,\n",
    "                    help='Name of run within the experiment')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Function to retrieve data from BigQuery\n",
    "def get_data():\n",
    "    logging.info(\"Downloading training data from BigQuery: {}, {}\".format(args.project_id, args.bq_table))\n",
    "    logging.info(\"Creating BigQuery client\")\n",
    "    bqclient = bigquery.Client(project=args.project_id)\n",
    "    \n",
    "    logging.info(\"Loading table data\")\n",
    "    table = bigquery.TableReference.from_string(args.bq_table)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    dataframe = rows.to_dataframe()\n",
    "    \n",
    "    logging.info(\"Preparing data for training\")\n",
    "    dataframe[\"isFraud\"] = dataframe[\"isFraud\"].astype(int)\n",
    "    dataframe.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1,inplace=True)\n",
    "    X = pd.concat([dataframe.drop('type', axis=1), pd.get_dummies(dataframe['type'])], axis=1)\n",
    "    y = X[['isFraud']]\n",
    "    X = X.drop(['isFraud'],axis=1)\n",
    "    \n",
    "    logging.info(\"Splitting data for training\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42, shuffle=True)\n",
    "    \n",
    "    logging.info(\"Finishing get_data\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(X_train, y_train):\n",
    "    logging.info(\"Start training ...\")\n",
    "    model = xgb.XGBClassifier(\n",
    "            scale_pos_weight=734,\n",
    "            max_depth=7\n",
    "            learning_rate=0.03289820323933852\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    logging.info(\"Training completed\")\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    logging.info(\"Preparing test data ...\")\n",
    "    data_test = xgb.DMatrix(X_test)\n",
    "    \n",
    "    logging.info(\"Getting test predictions ...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    logging.info(\"Evaluating predictions ...\")\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    logging.info(f\"Evaluation completed with weighted f1 score: {f1}\")\n",
    "    \n",
    "    logging.info(\"Finishing ...\")\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Create a run within the experiment\n",
    "aiplatform.init(experiment=args.experiment)\n",
    "aiplatform.start_run(args.run)\n",
    "\n",
    "with aiplatform.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=\"xgboost_training\"\n",
    ") as execution:\n",
    "    logging.info(\"Starting execution ...\")\n",
    "    X_train, X_test, y_train, y_test = get_data()\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # GCSFuse conversion\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if args.model_dir.startswith(gs_prefix):\n",
    "        args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "        dirpath = os.path.split(args.model_dir)[0]\n",
    "        if not os.path.isdir(dirpath):\n",
    "            os.makedirs(dirpath)\n",
    "\n",
    "    # Export the classifier to a file\n",
    "    gcs_model_path = os.path.join(args.model_dir, 'model.bst')\n",
    "    logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
    "    model.save_model(gcs_model_path)\n",
    "\n",
    "    logging.info(\"Saving metrics to {}/metrics.json\". format(args.model_dir))\n",
    "    gcs_metrics_path = os.path.join(args.model_dir, 'metrics.json')\n",
    "    with open(gcs_metrics_path, \"w\") as f:\n",
    "        f.write(json.dumps(metric_dict))\n",
    "\n",
    "aiplatform.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4f621-713b-4660-be46-9e48b6542023",
   "metadata": {},
   "source": [
    "### Store Training Script in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7e80589-4503-434c-9e5a-9de2b948db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/setup.py\n",
      "custom/setup.cfg\n",
      "custom/README.md\n",
      "custom/PKG-INFO\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.2 KiB/  2.2 KiB]                                                \n",
      "Operation completed over 1 objects/2.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_fraud.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57674bd2-5da2-447c-a8f5-86db683ae1d3",
   "metadata": {},
   "source": [
    "### Generate UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5487ff44-a96a-42ac-8e14-bcdf8b735c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0377c9-bafa-4ab5-9092-46827e9aa503",
   "metadata": {},
   "source": [
    "## Construct Custom Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cec830-5892-452c-aa9f-340525b6261a",
   "metadata": {},
   "source": [
    "Construct a pipeline for training a custom model using pre-built Google Cloud Pipeline Components for Vertex AI Training, as follows:\n",
    "\n",
    "1. Pipeline arguments, specify the locations of:\n",
    "- **python_package:** The custom training Python package.\n",
    "- **python_module:** The entry module in the package to execute.\n",
    "- **display_name:** The human readable resource name for generated resources\n",
    "- **bucket:** The Cloud Storage location to store model artifacts\n",
    "- **project:** The project ID.\n",
    "- **region:** The region.\n",
    "\n",
    "2. Use the prebuilt component CustomPythonPackageTrainingJobRunOp to train a custom model and upload the custom model as a Vertex AI Model resource, where:\n",
    "- The display name for the model.\n",
    "- The dataset is specified within the training package.\n",
    "- The python package are passed into the pipeline.\n",
    "- The command line arguments for the python package are hardcoded in the call to the component.\n",
    "- The command line arguments for the name of the experiment and run are hardcoded in the call to the component.\n",
    "- The training and serving containers are specified in the pipeline definition.\n",
    "- The component returns the model resource as outputs[\"model\"].\\\n",
    "\n",
    "Note: Since each component is executed as a graph node in its own execution context, you pass the parameter project for each component op, in constrast to doing a aip.init(project=project) if this was a Python script calling the SDK methods directly within the same execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d234d50c-17cb-482f-a5dc-cec000a16a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/custom_xgboost_training\".format(BUCKET_URI)\n",
    "EXPERIMENT_NAME = f\"xgboostfraud{UUID}\"\n",
    "BQ_TABLE = \"bq-experiments-350102.synthetic_financial_fraud.fraud_data\"\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, UUID)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"fraud-xgboost\",\n",
    "    description=\"Train and deploy a custom XGBoost model for fraud detection\",\n",
    ")\n",
    "def pipeline(\n",
    "    display_name: str,\n",
    "    python_package: str,\n",
    "    python_module: str,\n",
    "    bucket: str = PIPELINE_ROOT,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "):\n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "    _ = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        # Training\n",
    "        python_package_gcs_uri=python_package,\n",
    "        python_module_name=python_module,\n",
    "        container_uri=TRAIN_IMAGE,\n",
    "        staging_bucket=PIPELINE_ROOT,\n",
    "        args=[\n",
    "            \"--project-id=\" + PROJECT_ID,\n",
    "            \"--bq-table=\" + BQ_TABLE,\n",
    "            \"--model_dir=\" + MODEL_DIR,\n",
    "            EXPERIMENT_NAME,\n",
    "            \"--run\",\n",
    "            \"fraud-run-1\",\n",
    "        ],\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        # Serving - As part of this operation, the model is registered to Vertex AI\n",
    "        model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "        model_display_name=display_name,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7e86a-3434-423b-b66a-987b5aa274c8",
   "metadata": {},
   "source": [
    "## Compile and Execute The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cdc4e73-66e8-4723-9427-7acba57c1da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fraud-xgboost-20221115173902?project=402374189238\n",
      "PipelineJob projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/402374189238/locations/us-central1/pipelineJobs/fraud-xgboost-20221115173902 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [custompythonpackagetrainingjob-run].; Job (project_id = bq-experiments-350102, job_id = 5172341291081531392) is failed due to the above error.; Failed to handle the job: {project_number = 402374189238, job_id = 5172341291081531392}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/216460074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' rm -f custom_fraud_training.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     def submit(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [custompythonpackagetrainingjob-run].; Job (project_id = bq-experiments-350102, job_id = 5172341291081531392) is failed due to the above error.; Failed to handle the job: {project_number = 402374189238, job_id = 5172341291081531392}\"\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_xgboost_training.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"custom_xgboost\",\n",
    "    template_path=\"custom_xgboost_training.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"display_name\": \"fraud_\" + UUID,\n",
    "        \"python_package\": f\"{BUCKET_URI}/trainer_fraud.tar.gz\",\n",
    "        \"python_module\": \"trainer.task\",\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm -f custom_fraud_training.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e71a0-127e-4560-be4e-3e22993ff4b1",
   "metadata": {},
   "source": [
    "## View Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b44d29-5c64-4d15-9cb7-ad6b8f117769",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        EVAL_METRICS = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/evaluation_metrics\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            return EXECUTE_OUTPUT\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            return GCP_RESOURCES\n",
    "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
    "            ! gsutil cat $EVAL_METRICS\n",
    "            return EVAL_METRICS\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"custompythonpackagetrainingjob-run\")\n",
    "artifacts = print_pipeline_output(pipeline, \"custompythonpackagetrainingjob-run\")\n",
    "print(\"\\n\\n\")\n",
    "output = !gsutil cat $artifacts\n",
    "output = json.loads(output[0])\n",
    "model_id = output[\"artifacts\"][\"model\"][\"artifacts\"][0][\"metadata\"][\"resourceName\"]\n",
    "print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
